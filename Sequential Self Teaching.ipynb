{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Self Teaching Worked Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up\n",
    "\n",
    "Three features $(x1, x_2, x3)$ and two labels $(y_0, y_1)$\n",
    "\n",
    "The hypothesis space is given by:\n",
    "\n",
    "$$\n",
    "h_1 = [1, 1, 1] \\\\\n",
    "h_2 = [0, 1, 1] \\\\\n",
    "h_3 = [0, 0, 1] \\\\\n",
    "h_4 = [0, 0, 0]\n",
    "$$\n",
    "\n",
    "The learner's prior over hypotheses is uniform, $p_L(h') = 1/4 \\quad \\forall h' \\in h$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_boundary_hyp_space(n_features):\n",
    "    \"\"\"Creates a hypothesis space of concepts defined by a linear boundary\"\"\"\n",
    "    hyp_space = []\n",
    "    for i in range(n_features + 1):\n",
    "        hyp = [1 for _ in range(n_features)]\n",
    "        hyp[:i] = [0 for _ in range(i)]\n",
    "        hyp_space.append(hyp)\n",
    "    hyp_space = np.array(hyp_space)\n",
    "    return hyp_space\n",
    "\n",
    "# initialize model\n",
    "n_features = 3  # number of features\n",
    "features = np.arange(n_features)  # features\n",
    "n_labels = 2  # number of labels\n",
    "labels = np.arange(n_labels)  # labels\n",
    "hyp_space = create_boundary_hyp_space(n_features)\n",
    "n_hyp = len(hyp_space)  # number of hypotheses\n",
    "hyp_shape = (n_hyp, n_features, n_labels)  # shape of structures\n",
    "\n",
    "# set learner's prior p_L(h) to be uniform over hypotheses\n",
    "learner_prior = 1 / n_hyp * np.ones(hyp_shape)\n",
    "\n",
    "# set self-teaching posterior p_T(x|h) to be uniform over features\n",
    "self_teaching_prior = 1 / n_features * np.ones(hyp_shape)\n",
    "\n",
    "assert np.allclose(np.sum(learner_prior, axis=0), 1.0)\n",
    "assert np.allclose(np.sum(self_teaching_prior, axis=1), 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood $p(y|x, h)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lik = np.ones(hyp_shape)\n",
    "\n",
    "for i, hyp in enumerate(hyp_space):\n",
    "    for j, feature in enumerate(features):\n",
    "        for k, label in enumerate(labels):\n",
    "            if hyp[feature] == label:\n",
    "                lik[i, j, k] = 1\n",
    "            else:\n",
    "                lik[i, j, k] = 0\n",
    "                \n",
    "assert lik.shape == hyp_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the learner's posterior $p_L(h|x, y) \\propto p(y|x, h)p_T(x|h)p_L(h)$ is..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiply everything together and normalize\n",
    "learner_posterior = lik * self_teaching_prior * learner_prior\n",
    "learner_posterior = learner_posterior / np.sum(learner_posterior, axis=0)\n",
    "\n",
    "assert learner_posterior.shape == hyp_shape\n",
    "assert np.allclose(np.sum(learner_posterior, axis=0), 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the learner's posterior, we can calculate the probability that the self-teacher will teach each feature, i.e. $p_T(x|h)$, by the following equations:\n",
    "\n",
    "$$\n",
    "p_T(x,y|h') \\propto p_L(h'|x,y)p_T(x,y), \n",
    "$$\n",
    "\n",
    "where $p_T(x,y)$ is usually a uniform prior over $x,y$;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set teaching prior to be uniform over labels and features\n",
    "teaching_prior = 1 / (n_features * n_labels) * np.ones(hyp_shape)\n",
    "assert np.allclose(np.sum(teaching_prior, axis=(1, 2)), 1.0)\n",
    "\n",
    "# multiply with learner's posterior\n",
    "feature_label_posterior = learner_posterior * teaching_prior\n",
    "feature_label_posterior = (feature_label_posterior.T / \n",
    "                               np.sum(feature_label_posterior, axis=(1, 2))).T\n",
    "\n",
    "assert feature_label_posterior.shape == hyp_shape\n",
    "assert np.allclose(np.sum(feature_label_posterior, axis=(1, 2)), 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "p_T(x,y,h'|h) &= p_T(x,y|h')p_a(h'|h) \\\\\n",
    "&= p_T(x,y|h')p_L(h') = p_T(x,y,h'),\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $p_a(h'|h) = p_L(h')$ captures the self-teaching idea that the learner's posterior on $h'$ does not depend on the underlying true $h$, and thus, turns the selection probability $p_T(x,h')$ independent of $h$;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_teaching_hyp_prior = 1 / n_hyp * np.ones(hyp_shape)  # p(h'|h)\n",
    "assert np.allclose(np.sum(teaching_prior, axis=(1, 2)), 1.0)\n",
    "\n",
    "joint_self_teaching_posterior = feature_label_posterior * self_teaching_hyp_prior\n",
    "assert joint_self_teaching_posterior.shape == hyp_shape\n",
    "assert np.isclose(np.sum(joint_self_teaching_posterior), 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p_T(x|h) = p_T(x) = \\sum_y \\sum_{h'} p_T(x,y,h').$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.32467532  0.35064935  0.32467532]\n"
     ]
    }
   ],
   "source": [
    "self_teaching_posterior_original = np.sum(joint_self_teaching_posterior, axis=(0, 2))\n",
    "print(self_teaching_posterior_original)\n",
    "self_teaching_posterior = np.tile(np.tile(self_teaching_posterior_original, (n_labels, 1)).T, \n",
    "                                  (n_hyp, 1, 1))  # broadcast to be the same shape\n",
    "\n",
    "assert self_teaching_posterior.shape == hyp_shape\n",
    "assert np.allclose(np.sum(self_teaching_posterior, axis=1), 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can check that cooperative inference in self-teaching converges after 1 step because $p_T(x|h) = p_T(x)$, which is independent of $h$, which means\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p_L(h|x, y) &\\propto p(y|x, h)p_T(x|h)p(h) \\\\\n",
    "\\rightarrow p_L(h|x, y) &\\propto p(y|x, h)p_T(x)p(h) \\\\\n",
    "\\rightarrow p_L(h|x, y) &\\propto p(y|x, h)p(h).\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_learner_posterior = lik * self_teaching_posterior * learner_prior\n",
    "updated_learner_posterior = updated_learner_posterior / np.sum(updated_learner_posterior, axis=0)\n",
    "\n",
    "assert np.allclose(updated_learner_posterior, learner_posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look ahead\n",
    "\n",
    "Up until here, the self-teacher has considered the effectiveness of selecting a particular feature according to its immediate benefit. A self-teacher can also look ahead into the future to see what the consequence of this first selection and modify the selection probabiity according to a particular feature's future benefit. This can be done by the following steps:\n",
    "\n",
    "First, compute the look-ahead posterior\n",
    "\n",
    "$$\n",
    "p_L(h|x^{(2)}, y^{(2)}, x^{(1)}, y^{(1)}) \\propto p(y^{(2)}|h, x^{(2)}, x^{(1)}, y^{(1)})p_L(h|x^{(1)},y^{(1)}),\n",
    "$$\n",
    "\n",
    "where $x^{(2)}, y^{(2)}$ respectively denote the selection and outcome one more step into the future;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of computing the look-ahead posterior is to calculate the likelihood $$p(y^{(2)}|x^{(2)}, y^{(1)}, x^{(1)}, h)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_hyp_shape = (n_hyp, n_features, n_labels, n_features, n_labels)\n",
    "sequential_lik = np.ones(sequential_hyp_shape)\n",
    "\n",
    "for i, hyp in enumerate(hyp_space):\n",
    "    for j, feature_one in enumerate(features):\n",
    "        for k, label_one in enumerate(labels):\n",
    "            for l, feature_two in enumerate(features):\n",
    "                for m, label_two in enumerate(labels):\n",
    "                    if hyp[feature_one] == label_one and hyp[feature_two] == label_two:\n",
    "                        sequential_lik[i, j, k, l, m] = 1\n",
    "                    else:\n",
    "                        sequential_lik[i, j, k, l, m] = 0\n",
    "\n",
    "# code to double check sequential likelihood is calculated correctly\n",
    "sequential_lik_two = np.repeat(lik, n_features * n_labels).reshape(sequential_hyp_shape)\n",
    "# likelihood needs to calculate over having observed both features\n",
    "for i, feature in enumerate(features):\n",
    "    for j, label in enumerate(labels):\n",
    "        sequential_lik_two[:, :, :, i, j] = sequential_lik_two[:, i, j, : :] * \\\n",
    "            sequential_lik_two[:, :, :, i, j]\n",
    "            \n",
    "assert sequential_lik.shape == sequential_hyp_shape\n",
    "assert np.array_equal(sequential_lik, sequential_lik_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we calculate the sequential self teaching prior $$p_T(x^{(2)}|x^{(1)}, y^{(1)}, h)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set sequential self teaching prior to be uniform\n",
    "sequential_self_teaching_prior = 1 / n_features * np.ones(sequential_hyp_shape)\n",
    "\n",
    "assert np.allclose(np.sum(sequential_self_teaching_prior, axis=3), 1.0)\n",
    "\n",
    "# also consider setting the sequential self teaching prior to be \n",
    "# zero when the second feature is the same as the first)\n",
    "# sequential_self_teaching_prior = np.array(sequential_self_teaching_prior, copy=True)\n",
    "# for i, feature_one in enumerate(features):\n",
    "#     for j, feature_two in enumerate(features):\n",
    "#         if feature_one == feature_two:\n",
    "#             sequential_self_teaching_prior[:, i, :, j, :] = 0\n",
    "            \n",
    "# # normalize \n",
    "# sequential_self_teaching_prior  = sequential_self_teaching_prior / \\\n",
    "#     np.repeat(np.sum(sequential_self_teaching_prior, axis=3), n_features).reshape(\n",
    "#         sequential_hyp_shape)\n",
    "\n",
    "# assert np.allclose(np.sum(sequential_self_teaching_prior, axis=3), 1.0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/py36/lib/python3.6/site-packages/ipykernel/__main__.py:16: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "# expand posterior to same shape\n",
    "sequential_learner_posterior_one = np.repeat(learner_posterior, n_features * n_labels).reshape(\n",
    "    sequential_hyp_shape)\n",
    "sequential_learner_posterior_two = np.repeat(learner_posterior, n_features * n_labels).reshape(\n",
    "    sequential_hyp_shape)\n",
    "\n",
    "# re-order axes so that posterior matches along the same axes\n",
    "for i, feature in enumerate(features):\n",
    "    for j, label in enumerate(labels):\n",
    "        sequential_learner_posterior_one[:, :, :, i, j] = \\\n",
    "            sequential_learner_posterior_two[:, i, j, : :] \n",
    "            \n",
    "# compute look-ahead posterior\n",
    "lookahead_posterior = sequential_lik * sequential_self_teaching_prior * \\\n",
    "    sequential_learner_posterior_one\n",
    "    \n",
    "# normalize and set NaNs to zero\n",
    "lookahead_posterior = lookahead_posterior / np.nansum(lookahead_posterior, axis=0)\n",
    "lookahead_posterior = np.nan_to_num(lookahead_posterior)\n",
    "\n",
    "# check if posterior only contains zeros and ones\n",
    "assert lookahead_posterior.shape == sequential_hyp_shape\n",
    "assert np.array_equal(np.sum(lookahead_posterior, axis=0), \n",
    "                      (np.sum(lookahead_posterior, axis=0)).astype(bool))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "second, compute\n",
    "\n",
    "$$\n",
    "p_T(x^{(2)},y^{(2)}|h',x,y) \\propto p_L(h'|x^{(2)},y^{(2)},x,y)p_T(x^{(2)},y^{(2)}), \n",
    "$$\n",
    "\n",
    "where $p_T(x^{(2)},y^{(2)})$ is uniform over $x^{(2)},y^{(2)}$ as before;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/py36/lib/python3.6/site-packages/ipykernel/__main__.py:14: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "# set teaching prior to be uniform over labels and features\n",
    "sequential_teaching_prior = 1 / (n_features * n_labels) * np.ones(sequential_hyp_shape)\n",
    "\n",
    "assert np.allclose(np.sum(sequential_teaching_prior, axis=(3, 4)), 1.0)\n",
    "\n",
    "# multiply with learner's posterior\n",
    "sequential_feature_label_posterior = lookahead_posterior * sequential_teaching_prior\n",
    "sequential_feature_label_data_lik = np.sum(sequential_feature_label_posterior, axis=(3, 4))\n",
    "\n",
    "# normalize\n",
    "for i, hyp in enumerate(hyp_space):\n",
    "    for j, feature_one in enumerate(features):\n",
    "        for k, label_one in enumerate(labels):\n",
    "            sequential_feature_label_posterior[i, j, k, :, :] = \\\n",
    "                sequential_feature_label_posterior[i, j, k, :, :] / \\\n",
    "                    sequential_feature_label_data_lik[i, j, k]\n",
    "\n",
    "# turn nans to zero\n",
    "sequential_feature_label_posterior = np.nan_to_num(sequential_feature_label_posterior)\n",
    "\n",
    "# check if posterior only contains zeros and ones\n",
    "assert sequential_feature_label_posterior.shape == sequential_hyp_shape\n",
    "assert np.allclose(np.sum(sequential_feature_label_posterior, axis=(3, 4)), \n",
    "                    np.sum(sequential_feature_label_posterior, axis=(3, 4)).astype(bool)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "third, compute\n",
    "\n",
    "$$\n",
    "p_T(x^{(2)},y^{(2)},h'|x,y) = p_T(x^{(2)},y^{(2)}|h',x, y)p_L(h'|x, y);\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_joint_posterior = sequential_feature_label_posterior * sequential_learner_posterior_one\n",
    "sequential_joint_data_lik = np.sum(sequential_joint_posterior, axis=(0, 3, 4))\n",
    "\n",
    "# normalize\n",
    "for i, hyp in enumerate(hyp_space):\n",
    "    for j, feature_two in enumerate(features):\n",
    "        for k, label_two in enumerate(labels):\n",
    "            sequential_joint_posterior[i, :, :, j, k] = sequential_joint_posterior[i, :, :, j, k] / \\\n",
    "                sequential_joint_data_lik\n",
    "\n",
    "assert np.allclose(np.sum(sequential_joint_posterior, axis=(0, 3, 4)), 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "forth, compute the predictive distribution (not 100% sure if this is right)\n",
    "\n",
    "$$\n",
    "p_L(y|x) = \\sum_h p(y|h, x)p_L(h|x) = \\sum_h p(y|h, x)p_L(h);\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_predictives_original = np.sum(lik * learner_prior, axis=0) \n",
    "label_predictives = np.empty(sequential_hyp_shape)\n",
    "\n",
    "# do swapping axes trick\n",
    "for i, hyp in enumerate(hyp_space):\n",
    "    for j, feature_one in enumerate(features):\n",
    "        for k, label_one in enumerate(labels):\n",
    "            label_predictives[:, j, k] = label_predictives_original[j, k]\n",
    "            \n",
    "assert np.allclose(np.sum(label_predictives, axis=2), 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fifth, compute\n",
    "\n",
    "$$\n",
    "p_T(x^{(2)},y^{(2)},h',y|x) = p_T(x^{(2)},y^{(2)},h'|x, y)p_L(y|x);\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_full_joint_posterior = sequential_joint_posterior * label_predictives\n",
    "\n",
    "assert np.allclose(np.sum(sequential_full_joint_posterior, axis=(0, 2, 3, 4)), 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sixth, compute\n",
    "\n",
    "$$\n",
    "p_T(x^{(2)}|x) = \\sum_{y^{(2)}} \\sum_{h'} \\sum_y p_T(x^{(2)},y^{(2)},h',y|x);\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1.]\n",
      "[ 0.07615112  0.11511216  0.13341204]\n"
     ]
    }
   ],
   "source": [
    "sequential_conditional_feature_prob = np.sum(sequential_full_joint_posterior, axis=(0, 2, 4))\n",
    "\n",
    "assert np.allclose(np.sum(sequential_conditional_feature_prob, axis=1), 1.0)\n",
    "\n",
    "print(np.sum(sequential_conditional_feature_prob, axis=1))\n",
    "print(sequential_conditional_feature_prob[0] * self_teaching_posterior_original[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lastly, combine the benefit from $x^{(2)}$:\n",
    "\n",
    "$$\n",
    "p_T(x^{(2)}, x) = p_T(x^{(2)}|x) p_T(x) \\\\\n",
    "p^{(2)}_T(x) = \\sum_{x^{(2)}} p_T(x^{(2)},x).\n",
    "$$\n",
    "\n",
    "(Written out this way, it seems like look-ahead will never have any effect. So, I am probably doing something wrong.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.32467532  0.35064935  0.32467532]\n"
     ]
    }
   ],
   "source": [
    "sequential_joint_feature_prob = sequential_conditional_feature_prob.T * self_teaching_posterior_original\n",
    "assert np.isclose(np.sum(sequential_joint_feature_prob, axis=(0, 1)), 1.0)\n",
    "\n",
    "sequential_self_teaching_prob = np.sum(sequential_joint_feature_prob, axis=0)\n",
    "\n",
    "# normalize\n",
    "sequential_self_teaching_prob = sequential_self_teaching_prob / \\\n",
    "    np.sum(sequential_self_teaching_prob)\n",
    "    \n",
    "print(sequential_self_teaching_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
